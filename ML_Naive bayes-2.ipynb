{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847adbb7-db76-4977-981e-bb4bb638709c",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd065e-f3c9-4607-a19d-30f13d51dd2b",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "Probability that an employee is a smoker given that he/she uses the health insurance plan:\n",
    "\n",
    "To calculate this probability, we can use Bayes' theorem. Let's denote the events as follows:\n",
    "\n",
    "A: Employee uses the health insurance plan\n",
    "\n",
    "B: Employee is a smoker\n",
    "\n",
    "We are given P(A) = 70% = 0.7 (probability of using the health insurance plan) and P(B|A) = 40% = 0.4 (probability of being a smoker given that the employee uses the plan).\n",
    "\n",
    "Using Bayes' theorem:\n",
    "\n",
    "P(B|A) = P(A|B) * P(B) / P(A)\n",
    "\n",
    "where:\n",
    "\n",
    "P(B|A) is the probability of being a smoker given that the employee uses the health insurance plan.\n",
    "\n",
    "P(A|B) is the probability of using the health insurance plan given that the employee is a smoker. Since this information is not given, we'll assume it is the same as P(A), which is 0.7.\n",
    "\n",
    "P(B) is the probability of being a smoker, which we need to calculate.\n",
    "\n",
    "P(B) = P(A and B) + P(A' and B) (where A' is the complement of A, i.e., not using the health insurance plan)\n",
    "\n",
    "Since we don't have information about the overall percentage of smokers in the company, let's assume it's 20% = 0.2.\n",
    "\n",
    "P(B) = (P(A) * P(B|A)) + (P(A') * P(B|A')) = (0.7 * 0.4) + (0.3 * x) = 0.28 + 0.3x\n",
    "\n",
    "\n",
    "Now, we know that the sum of probabilities in a sample space is equal to 1:\n",
    "\n",
    "P(A) + P(A') = 1\n",
    "\n",
    "0.7 + 0.3 = 1\n",
    "\n",
    "0.3 = 1 - 0.7\n",
    "\n",
    "0.3 = 0.3x\n",
    "\n",
    "x = 1\n",
    "\n",
    "Therefore, P(B) = 0.28 + 0.3 = 0.58\n",
    "\n",
    "Now, we can calculate P(B|A) using Bayes' theorem:\n",
    "\n",
    "P(B|A) = P(A|B) * P(B) / P(A)\n",
    "\n",
    "P(B|A) = 0.7 * 0.4 / 0.7\n",
    "\n",
    "P(B|A) = 0.4\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.4 or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768ab2e-88b4-43b3-a1dd-6c72edbb4923",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491180f2-5135-481c-9aa5-c7f7f6e92a5f",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c8edb9-4e1e-477c-a364-b94ab71036a4",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "Difference between Bernoulli Naive Bayes and Multinomial Naive Bayes:\n",
    "\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm commonly used in text classification and other discrete feature-based problems. The main difference between them lies in how they handle the feature representation:\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Suitable for binary feature data.\n",
    "\n",
    "Assumes that each feature is binary (present or absent) and doesn't consider the frequency of the feature occurrences.\n",
    "It uses binary values (0 or 1) to represent the absence or presence of a particular feature in the input.\n",
    "Often used for tasks like spam classification, where the presence or absence of certain words is essential, but their frequency is not considered.\n",
    "\n",
    "\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Suitable for discrete count-based feature data.\n",
    "\n",
    "Assumes that each feature represents the count or frequency of a particular word or term in the input.\n",
    "\n",
    "It uses integer counts (non-negative integers) to represent the frequency of feature occurrences.\n",
    "\n",
    "Often used for tasks like document classification, sentiment analysis, or text categorization, where the frequency of words is crucial.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes works well when dealing with binary features, while Multinomial Naive Bayes is appropriate when working with count-based or frequency-based discrete features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7fe27-1fe2-442b-9990-ddf2b4958328",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acec684-dfcf-48b9-919d-91939ae4fe94",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d565c-385c-439b-9285-61ed94caf9fa",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "Bernoulli Naive Bayes handles missing values by assuming that missing features are equivalent to features that are not present (binary value 0). In other words, when a particular feature value is missing for a data point, it is treated as if that feature is not present for that data point.\n",
    "\n",
    "During the training phase of the Bernoulli Naive Bayes classifier, it learns the probabilities of each feature being 0 or 1 for each class based on the available data. If a feature is missing in the training data for a particular class, the classifier will not consider that feature when making predictions for instances of that class.\n",
    "\n",
    "During the prediction phase, when a new data point with missing values is encountered, the Bernoulli Naive Bayes classifier will ignore the missing features and make predictions based on the available features. If a feature is missing, its corresponding probability of being 0 will be used.\n",
    "\n",
    "It's important to note that how missing values are handled can impact the performance of the classifier. If there are many missing values in the dataset, it may lead to biased predictions or suboptimal performance. It is recommended to handle missing values appropriately before using the Bernoulli Naive Bayes classifier to ensure accurate and reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68aa9b-06e7-48ba-9454-ab662e3c7fc2",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d8454-0f91-4c58-9047-9330eaa2c9b0",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc33b3-63e6-4865-85f7-d4d8babbb1e0",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes the features in the data follow a Gaussian (normal) distribution. It is commonly used for continuous or real-valued features.\n",
    "\n",
    "In the context of multi-class classification, Gaussian Naive Bayes can be extended to handle multiple classes by applying the Bayes theorem independently for each class and selecting the class with the highest probability as the predicted class for a given input.\n",
    "\n",
    "Here's a brief overview of how Gaussian Naive Bayes works for multi-class classification:\n",
    "\n",
    "Training Phase:\n",
    "\n",
    "For each class, the mean and variance of each feature are calculated based on the training data belonging to that class.\n",
    "The mean and variance are used to describe the Gaussian distribution of each feature for each class.\n",
    "\n",
    "Prediction Phase:\n",
    "\n",
    "Given a new data point with continuous feature values, the probability of the data point belonging to each class is calculated independently using the Gaussian probability density function for each feature.\n",
    "The class with the highest probability is assigned as the predicted class for the new data point.\n",
    "It's essential to note that while Gaussian Naive Bayes is suitable for multi-class classification, it makes the assumption that the features within each class follow a Gaussian distribution, which may not always hold true in practice. If the features do not follow a Gaussian distribution, the performance of Gaussian Naive Bayes may be suboptimal compared to other algorithms that can handle non-Gaussian distributions.#Answer\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes the features in the data follow a Gaussian (normal) distribution. It is commonly used for continuous or real-valued features.\n",
    "\n",
    "In the context of multi-class classification, Gaussian Naive Bayes can be extended to handle multiple classes by applying the Bayes theorem independently for each class and selecting the class with the highest probability as the predicted class for a given input.\n",
    "\n",
    "Here's a brief overview of how Gaussian Naive Bayes works for multi-class classification:\n",
    "\n",
    "Training Phase:\n",
    "\n",
    "For each class, the mean and variance of each feature are calculated based on the training data belonging to that class.\n",
    "The mean and variance are used to describe the Gaussian distribution of each feature for each class.\n",
    "Prediction Phase:\n",
    "\n",
    "Given a new data point with continuous feature values, the probability of the data point belonging to each class is calculated independently using the Gaussian probability density function for each feature.\n",
    "The class with the highest probability is assigned as the predicted class for the new data point.\n",
    "It's essential to note that while Gaussian Naive Bayes is suitable for multi-class classification, it makes the assumption that the features within each class follow a Gaussian distribution, which may not always hold true in practice. If the features do not follow a Gaussian distribution, the performance of Gaussian Naive Bayes may be suboptimal compared to other algorithms that can handle non-Gaussian distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbaf43-6ff9-49ad-b189-2a56c6f188e8",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83dbde1-120b-428e-9820-73b17987c381",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "\n",
    "Data preparation:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "\n",
    "Report the following performance metrics for each classifier:\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Precision\n",
    "\n",
    "Recall\n",
    "\n",
    "F1 score\n",
    "\n",
    "Discussion:\n",
    "\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc11361c-a768-49f5-8613-a1328d343ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Results:\n",
      "Accuracy: 0.8839382742881983\n",
      "Precision: 0.8813357185450209\n",
      "Recall: 0.815223386651958\n",
      "F1 Score: 0.8469914040114614\n",
      "\n",
      "Multinomial Naive Bayes Results:\n",
      "Accuracy: 0.786350793305803\n",
      "Precision: 0.7323628219484882\n",
      "Recall: 0.7214561500275786\n",
      "F1 Score: 0.7268685746040567\n",
      "\n",
      "Gaussian Naive Bayes Results:\n",
      "Accuracy: 0.8217778743751358\n",
      "Precision: 0.7004440855874041\n",
      "Recall: 0.9569773855488142\n",
      "F1 Score: 0.8088578088578089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Answer\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "column_names = [f\"Feature_{i}\" for i in range(57)] + [\"IsSpam\"]\n",
    "data = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Step 2: Split the data into features (X) and labels (y)\n",
    "X = data.drop(\"IsSpam\", axis=1)\n",
    "y = data[\"IsSpam\"]\n",
    "\n",
    "# Step 3: Implement Naive Bayes classifiers and evaluate with 10-fold cross-validation\n",
    "classifiers = {\n",
    "    \"Bernoulli Naive Bayes\": BernoulliNB(),\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "}\n",
    "\n",
    "performance_metrics = {}\n",
    "for name, clf in classifiers.items():\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=10)\n",
    "    \n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "\n",
    "    performance_metrics[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "    }\n",
    "\n",
    "# Step 4: Print the results\n",
    "for name, metrics in performance_metrics.items():\n",
    "    print(f\"{name} Results:\")\n",
    "    print(\"Accuracy:\", metrics[\"Accuracy\"])\n",
    "    print(\"Precision:\", metrics[\"Precision\"])\n",
    "    print(\"Recall:\", metrics[\"Recall\"])\n",
    "    print(\"F1 Score:\", metrics[\"F1 Score\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c6469c-811c-4261-854e-d172b6171204",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Discussion:\n",
    "\n",
    "Based on the results obtained from 10-fold cross-validation, you can analyze the performance metrics of each Naive Bayes variant. The one that performs the best in terms of accuracy, precision, recall, and F1 score would be considered the most effective for this specific dataset.\n",
    "\n",
    "Generally, Bernoulli Naive Bayes works well when the features are binary (e.g., word occurrence), and Multinomial Naive Bayes is suitable when dealing with integer counts (e.g., word frequencies). Gaussian Naive Bayes is appropriate for continuous data assuming a Gaussian distribution.\n",
    "\n",
    "In this specific case, we cannot conclusively determine which variant of Naive Bayes performs the best without examining the results. The choice depends on the nature of the features and the underlying distribution of the data. If the features are binary or binary-like, Bernoulli Naive Bayes might perform well. If the features are integer counts, Multinomial Naive Bayes may be more appropriate. Gaussian Naive Bayes may perform well if the features are continuous and follow a Gaussian distribution.\n",
    "\n",
    "Limitations of Naive Bayes:\n",
    "\n",
    "Naive Bayes assumes that features are conditionally independent given the class, which may not be true in real-world datasets.\n",
    "It tends to struggle with rare or unseen feature combinations in the training data since it calculates probabilities for individual features.\n",
    "\n",
    "Sensitivity to irrelevant features: Naive Bayes can be influenced by irrelevant features, affecting its performance.\n",
    "The assumption of normal distribution in Gaussian Naive Bayes may not hold for some datasets.\n",
    "\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "In this experiment, we implemented and compared three variants of Naive Bayes classifiers on the \"Spambase Data Set\" using 10-fold cross-validation. The best-performing variant depends on the specific dataset and the nature of its features.\n",
    "\n",
    "Based on the performance metrics obtained, we can identify the most suitable variant for this dataset. Additionally, we discussed the limitations of Naive Bayes, which should be taken into consideration when using this method for classification tasks.\n",
    "\n",
    "For future work, you may consider the following:\n",
    "\n",
    "Experiment with different feature engineering techniques to improve the performance of Naive Bayes classifiers.\n",
    "\n",
    "Explore other classification algorithms, such as decision trees, random forests, or support vector machines, to see if they outperform Naive Bayes on this dataset.\n",
    "\n",
    "Consider balancing the dataset if there is a significant class imbalance to avoid biased results.\n",
    "\n",
    "Tune hyperparameters of the classifiers to potentially improve their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd56842-96dd-463e-9a53-abe5f7acc2c6",
   "metadata": {},
   "source": [
    "                       -------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
